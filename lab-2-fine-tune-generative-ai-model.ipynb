{"metadata":{"availableInstances":[{"_defaultOrder":0,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.t3.medium","vcpuNum":2},{"_defaultOrder":1,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.t3.large","vcpuNum":2},{"_defaultOrder":2,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.t3.xlarge","vcpuNum":4},{"_defaultOrder":3,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.t3.2xlarge","vcpuNum":8},{"_defaultOrder":4,"_isFastLaunch":true,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5.large","vcpuNum":2},{"_defaultOrder":5,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5.xlarge","vcpuNum":4},{"_defaultOrder":6,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5.2xlarge","vcpuNum":8},{"_defaultOrder":7,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5.4xlarge","vcpuNum":16},{"_defaultOrder":8,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5.8xlarge","vcpuNum":32},{"_defaultOrder":9,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5.12xlarge","vcpuNum":48},{"_defaultOrder":10,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5.16xlarge","vcpuNum":64},{"_defaultOrder":11,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5.24xlarge","vcpuNum":96},{"_defaultOrder":12,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.m5d.large","vcpuNum":2},{"_defaultOrder":13,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.m5d.xlarge","vcpuNum":4},{"_defaultOrder":14,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.m5d.2xlarge","vcpuNum":8},{"_defaultOrder":15,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.m5d.4xlarge","vcpuNum":16},{"_defaultOrder":16,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.m5d.8xlarge","vcpuNum":32},{"_defaultOrder":17,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.m5d.12xlarge","vcpuNum":48},{"_defaultOrder":18,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.m5d.16xlarge","vcpuNum":64},{"_defaultOrder":19,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.m5d.24xlarge","vcpuNum":96},{"_defaultOrder":20,"_isFastLaunch":false,"category":"General purpose","gpuNum":0,"hideHardwareSpecs":true,"memoryGiB":0,"name":"ml.geospatial.interactive","supportedImageNames":["sagemaker-geospatial-v1-0"],"vcpuNum":0},{"_defaultOrder":21,"_isFastLaunch":true,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":4,"name":"ml.c5.large","vcpuNum":2},{"_defaultOrder":22,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":8,"name":"ml.c5.xlarge","vcpuNum":4},{"_defaultOrder":23,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.c5.2xlarge","vcpuNum":8},{"_defaultOrder":24,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.c5.4xlarge","vcpuNum":16},{"_defaultOrder":25,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":72,"name":"ml.c5.9xlarge","vcpuNum":36},{"_defaultOrder":26,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":96,"name":"ml.c5.12xlarge","vcpuNum":48},{"_defaultOrder":27,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":144,"name":"ml.c5.18xlarge","vcpuNum":72},{"_defaultOrder":28,"_isFastLaunch":false,"category":"Compute optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.c5.24xlarge","vcpuNum":96},{"_defaultOrder":29,"_isFastLaunch":true,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g4dn.xlarge","vcpuNum":4},{"_defaultOrder":30,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g4dn.2xlarge","vcpuNum":8},{"_defaultOrder":31,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g4dn.4xlarge","vcpuNum":16},{"_defaultOrder":32,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g4dn.8xlarge","vcpuNum":32},{"_defaultOrder":33,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g4dn.12xlarge","vcpuNum":48},{"_defaultOrder":34,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g4dn.16xlarge","vcpuNum":64},{"_defaultOrder":35,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":61,"name":"ml.p3.2xlarge","vcpuNum":8},{"_defaultOrder":36,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":244,"name":"ml.p3.8xlarge","vcpuNum":32},{"_defaultOrder":37,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":488,"name":"ml.p3.16xlarge","vcpuNum":64},{"_defaultOrder":38,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.p3dn.24xlarge","vcpuNum":96},{"_defaultOrder":39,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.r5.large","vcpuNum":2},{"_defaultOrder":40,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.r5.xlarge","vcpuNum":4},{"_defaultOrder":41,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.r5.2xlarge","vcpuNum":8},{"_defaultOrder":42,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.r5.4xlarge","vcpuNum":16},{"_defaultOrder":43,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.r5.8xlarge","vcpuNum":32},{"_defaultOrder":44,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.r5.12xlarge","vcpuNum":48},{"_defaultOrder":45,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":512,"name":"ml.r5.16xlarge","vcpuNum":64},{"_defaultOrder":46,"_isFastLaunch":false,"category":"Memory Optimized","gpuNum":0,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.r5.24xlarge","vcpuNum":96},{"_defaultOrder":47,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":16,"name":"ml.g5.xlarge","vcpuNum":4},{"_defaultOrder":48,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":32,"name":"ml.g5.2xlarge","vcpuNum":8},{"_defaultOrder":49,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":64,"name":"ml.g5.4xlarge","vcpuNum":16},{"_defaultOrder":50,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":128,"name":"ml.g5.8xlarge","vcpuNum":32},{"_defaultOrder":51,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":1,"hideHardwareSpecs":false,"memoryGiB":256,"name":"ml.g5.16xlarge","vcpuNum":64},{"_defaultOrder":52,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":192,"name":"ml.g5.12xlarge","vcpuNum":48},{"_defaultOrder":53,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":4,"hideHardwareSpecs":false,"memoryGiB":384,"name":"ml.g5.24xlarge","vcpuNum":96},{"_defaultOrder":54,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":768,"name":"ml.g5.48xlarge","vcpuNum":192},{"_defaultOrder":55,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4d.24xlarge","vcpuNum":96},{"_defaultOrder":56,"_isFastLaunch":false,"category":"Accelerated computing","gpuNum":8,"hideHardwareSpecs":false,"memoryGiB":1152,"name":"ml.p4de.24xlarge","vcpuNum":96}],"colab":{"name":"Fine-tune a language model","provenance":[]},"instance_type":"ml.m5.2xlarge","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tune a Generative AI Model for Dialogue Summarization","metadata":{"tags":[]}},{"cell_type":"markdown","source":"In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics.","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.21.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:14.492855Z","iopub.execute_input":"2024-09-08T13:21:14.493507Z","iopub.status.idle":"2024-09-08T13:21:31.672605Z","shell.execute_reply.started":"2024-09-08T13:21:14.493472Z","shell.execute_reply":"2024-09-08T13:21:31.671409Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.2)\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:31.674527Z","iopub.execute_input":"2024-09-08T13:21:31.674877Z","iopub.status.idle":"2024-09-08T13:21:38.123013Z","shell.execute_reply.started":"2024-09-08T13:21:31.674841Z","shell.execute_reply":"2024-09-08T13:21:38.122181Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"huggingface_dataset_name = 'knkarthick/dialogsum'\n\ndataset = load_dataset(huggingface_dataset_name)\n\ndataset","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:38.124149Z","iopub.execute_input":"2024-09-08T13:21:38.124724Z","iopub.status.idle":"2024-09-08T13:21:40.763031Z","shell.execute_reply.started":"2024-09-08T13:21:38.124689Z","shell.execute_reply":"2024-09-08T13:21:40.761858Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 12460\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 500\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1500\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{"tags":[]}},{"cell_type":"code","source":"model_name = 'google/flan-t5-base'\n\noriginal_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:40.765354Z","iopub.execute_input":"2024-09-08T13:21:40.765688Z","iopub.status.idle":"2024-09-08T13:21:44.276522Z","shell.execute_reply.started":"2024-09-08T13:21:40.765629Z","shell.execute_reply":"2024-09-08T13:21:44.275521Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f'trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%'\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:44.277765Z","iopub.execute_input":"2024-09-08T13:21:44.278096Z","iopub.status.idle":"2024-09-08T13:21:44.286635Z","shell.execute_reply.started":"2024-09-08T13:21:44.278063Z","shell.execute_reply":"2024-09-08T13:21:44.285734Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"trainable model parameters: 247577856\nall model parameters: 247577856\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"index = 42\n\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninputs = tokenizer(prompt, return_tensors='pt')\noutput = tokenizer.decode(\n    original_model.generate(inputs['input_ids'], max_new_tokens=128)[0], \n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:44.288011Z","iopub.execute_input":"2024-09-08T13:21:44.288784Z","iopub.status.idle":"2024-09-08T13:21:45.142242Z","shell.execute_reply.started":"2024-09-08T13:21:44.288740Z","shell.execute_reply":"2024-09-08T13:21:45.141280Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n\nSummarize the following conversation.\n\n#Person1#: I don't know how to adjust my life. Would you give me a piece of advice?\n#Person2#: You look a bit pale, don't you?\n#Person1#: Yes, I can't sleep well every night.\n#Person2#: You should get plenty of sleep.\n#Person1#: I drink a lot of wine.\n#Person2#: If I were you, I wouldn't drink too much.\n#Person1#: I often feel so tired.\n#Person2#: You better do some exercise every morning.\n#Person1#: I sometimes find the shadow of death in front of me.\n#Person2#: Why do you worry about your future? You're very young, and you'll make great contribution to the world. I hope you take my advice.\n\nSummary:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nPerson1: I'm not sure how to adjust my life.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2'></a>\n## 2 - Perform Full Fine-Tuning","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.1'></a>\n### 2.1 - Preprocess the Dialog-Summary Dataset\n\nYou need to convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary` as follows:\n\nTraining prompt (dialogue):\n```\nSummarize the following conversation.\n\n    Chris: This is his part of the conversation.\n    Antje: This is her part of the conversation.\n    \nSummary: \n```\n\nTraining response (summary):\n```\nBoth Chris and Antje participated in the conversation.\n```\n\nThen preprocess the prompt-response dataset into tokens and pull out their `input_ids` (1 per token).","metadata":{"tags":[]}},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Summarize the following conversation.\\n\\n'\n    end_prompt = '\\n\\nSummary: '\n    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# The dataset actually contains 3 diff splits: train, validation, test.\n# The tokenize_function code is handling all data across all splits in batches.\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:45.143605Z","iopub.execute_input":"2024-09-08T13:21:45.144095Z","iopub.status.idle":"2024-09-08T13:21:45.189727Z","shell.execute_reply.started":"2024-09-08T13:21:45.144048Z","shell.execute_reply":"2024-09-08T13:21:45.188819Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:45.190920Z","iopub.execute_input":"2024-09-08T13:21:45.191232Z","iopub.status.idle":"2024-09-08T13:21:45.204808Z","shell.execute_reply.started":"2024-09-08T13:21:45.191190Z","shell.execute_reply":"2024-09-08T13:21:45.203863Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Check the shapes of all three parts of the dataset:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(f'Shapes of the datasets:')\nprint(f\"Training: {tokenized_datasets['train'].shape}\")\nprint(f\"Validation: {tokenized_datasets['validation'].shape}\")\nprint(f\"Test: {tokenized_datasets['test'].shape}\")\n\nprint(tokenized_datasets)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:45.206021Z","iopub.execute_input":"2024-09-08T13:21:45.206450Z","iopub.status.idle":"2024-09-08T13:21:45.212524Z","shell.execute_reply.started":"2024-09-08T13:21:45.206408Z","shell.execute_reply":"2024-09-08T13:21:45.211667Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Shapes of the datasets:\nTraining: (125, 2)\nValidation: (5, 2)\nTest: (15, 2)\nDatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 125\n    })\n    validation: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 5\n    })\n    test: Dataset({\n        features: ['input_ids', 'labels'],\n        num_rows: 15\n    })\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n\ninstruct_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-5,\n    num_train_epochs=10,\n    weight_decay=0.01,\n    per_device_train_batch_size=4,  # batch size for train\n    per_device_eval_batch_size=4,  # batch size for eval\n    fp16=True\n)\n\ntrainer = Trainer(\n    model=instruct_model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation']\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:45.216164Z","iopub.execute_input":"2024-09-08T13:21:45.216509Z","iopub.status.idle":"2024-09-08T13:21:49.945582Z","shell.execute_reply.started":"2024-09-08T13:21:45.216477Z","shell.execute_reply":"2024-09-08T13:21:49.944806Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:21:49.946661Z","iopub.execute_input":"2024-09-08T13:21:49.946966Z","iopub.status.idle":"2024-09-08T13:25:57.929079Z","shell.execute_reply.started":"2024-09-08T13:21:49.946934Z","shell.execute_reply":"2024-09-08T13:25:57.928192Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msvir\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240908_132153-5gtcsicj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/svir/huggingface/runs/5gtcsicj' target=\"_blank\">warm-night-40</a></strong> to <a href='https://wandb.ai/svir/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/svir/huggingface' target=\"_blank\">https://wandb.ai/svir/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/svir/huggingface/runs/5gtcsicj' target=\"_blank\">https://wandb.ai/svir/huggingface/runs/5gtcsicj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [320/320 03:46, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=320, training_loss=0.0, metrics={'train_runtime': 247.9547, 'train_samples_per_second': 5.041, 'train_steps_per_second': 1.291, 'total_flos': 855946690560000.0, 'train_loss': 0.0, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"code","source":"index = 42\ndialogue = dataset['test'][index]['dialogue']\nhuman_baseline_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary:\n\"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids.cpu(), generation_config=GenerationConfig(max_new_tokens=128, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:25:57.930320Z","iopub.execute_input":"2024-09-08T13:25:57.930657Z","iopub.status.idle":"2024-09-08T13:26:00.640961Z","shell.execute_reply.started":"2024-09-08T13:25:57.930617Z","shell.execute_reply":"2024-09-08T13:26:00.639948Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\nPerson1: I'm not sure how to adjust my life.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\n#Person1#: I'm not sure how to manage my life. #Person2: I'm not sure. #Person1: I'm not sure. #Person2: I'm not sure. #Person1: I drink a lot of wine. #Person2: I'm not sure. #Person1: I'm not sure. #Person1: I'm not sure. #Person2: I'm not sure.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='2.4'></a>\n### 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\n\nThe [ROUGE metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:00.642069Z","iopub.execute_input":"2024-09-08T13:26:00.642410Z","iopub.status.idle":"2024-09-08T13:26:01.627743Z","shell.execute_reply.started":"2024-09-08T13:26:00.642368Z","shell.execute_reply":"2024-09-08T13:26:01.626611Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Generate the outputs for the sample of the test dataset (only 10 dialogues and summaries to save time), and save the results.","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\n\nfor _, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    original_model_outputs = original_model.generate(input_ids=input_ids.cpu(), generation_config=GenerationConfig(max_new_tokens=128))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n    original_model_summaries.append(original_model_text_output)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n    instruct_model_summaries.append(instruct_model_text_output)\n    \nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:01.629130Z","iopub.execute_input":"2024-09-08T13:26:01.629436Z","iopub.status.idle":"2024-09-08T13:26:18.447466Z","shell.execute_reply.started":"2024-09-08T13:26:01.629397Z","shell.execute_reply":"2024-09-08T13:26:18.446323Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0     #Person1#: I need to take a dictation for you.   \n1     #Person1#: I need to take a dictation for you.   \n2     #Person1#: I need to take a dictation for you.   \n3  The traffic jam at the Carrefour intersection ...   \n4  The traffic jam at the Carrefour intersection ...   \n5  The traffic jam at the Carrefour intersection ...   \n6               Masha and Hero are getting divorced.   \n7               Masha and Hero are getting divorced.   \n8               Masha and Hero are getting divorced.   \n9  #Person1#: Happy birthday, Brian. #Person2#: I...   \n\n                            instruct_model_summaries  \n0  A memo is being sent to all employees by the f...  \n1  Employees are required to sign a memo re: the ...  \n2  Requests all employees to take a dictation of ...  \n3  The person who is driving to work is a person ...  \n4  The person is trying to find a different way t...  \n5                   The driver of the car is a jerk.  \n6               Masha and Hero are getting divorced.  \n7               Masha and Hero are getting divorced.  \n8               Masha and Hero are getting divorced.  \n9                   #Person1: Happy birthday, Brian.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n      <td>A memo is being sent to all employees by the f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n      <td>Employees are required to sign a memo re: the ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n      <td>Requests all employees to take a dictation of ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n      <td>The person who is driving to work is a person ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n      <td>The person is trying to find a different way t...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n      <td>The driver of the car is a jerk.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n      <td>#Person1: Happy birthday, Brian.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"original_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:18.448855Z","iopub.execute_input":"2024-09-08T13:26:18.449235Z","iopub.status.idle":"2024-09-08T13:26:18.898846Z","shell.execute_reply.started":"2024-09-08T13:26:18.449190Z","shell.execute_reply":"2024-09-08T13:26:18.897952Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.24353538210155856, 'rouge2': 0.1181436538994854, 'rougeL': 0.2225, 'rougeLsum': 0.22342236467236465}\nINSTRUCT MODEL:\n{'rouge1': 0.2856756819764339, 'rouge2': 0.1094229916450083, 'rougeL': 0.24726263227015108, 'rougeLsum': 0.24842272058813414}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The file `data/dialogue-summary-training-results.csv` contains a pre-populated list of all model results which you can use to evaluate on a larger section of data. Let's do that for each of the models:","metadata":{}},{"cell_type":"code","source":"# results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\nresults = df.copy()\n\nhuman_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:18.899855Z","iopub.execute_input":"2024-09-08T13:26:18.900227Z","iopub.status.idle":"2024-09-08T13:26:19.342773Z","shell.execute_reply.started":"2024-09-08T13:26:18.900192Z","shell.execute_reply":"2024-09-08T13:26:19.341853Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.24353538210155856, 'rouge2': 0.1181436538994854, 'rougeL': 0.2225, 'rougeLsum': 0.22342236467236465}\nINSTRUCT MODEL:\n{'rouge1': 0.2856756819764339, 'rouge2': 0.1094229916450083, 'rougeL': 0.24726263227015108, 'rougeLsum': 0.24842272058813414}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The results show substantial improvement in all ROUGE metrics:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(instruct_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:19.344168Z","iopub.execute_input":"2024-09-08T13:26:19.344579Z","iopub.status.idle":"2024-09-08T13:26:19.353772Z","shell.execute_reply.started":"2024-09-08T13:26:19.344534Z","shell.execute_reply":"2024-09-08T13:26:19.352550Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Absolute percentage improvement of INSTRUCT MODEL over HUMAN BASELINE\nrouge1: 4.21%\nrouge2: -0.87%\nrougeL: 2.48%\nrougeLsum: 2.50%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3'></a>\n## 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n\nNow, let's perform **Parameter Efficient Fine-Tuning (PEFT)** fine-tuning as opposed to \"full fine-tuning\" as you did above. PEFT is a form of instruction fine-tuning that is much more efficient than full fine-tuning - with comparable evaluation results as you will see soon. \n\nPEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n\nThat said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.","metadata":{}},{"cell_type":"markdown","source":"<a name='3.1'></a>\n### 3.1 - Setup the PEFT/LoRA model for Fine-Tuning\n\nYou need to set up the PEFT/LoRA model for fine-tuning with a new layer/parameter adapter. Using PEFT/LoRA, you are freezing the underlying LLM and only training the adapter. Have a look at the LoRA configuration below. Note the rank (`r`) hyper-parameter, which defines the rank/dimension of the adapter to be trained.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:19.354940Z","iopub.execute_input":"2024-09-08T13:26:19.355303Z","iopub.status.idle":"2024-09-08T13:26:19.378821Z","shell.execute_reply.started":"2024-09-08T13:26:19.355261Z","shell.execute_reply":"2024-09-08T13:26:19.377972Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"peft_model = get_peft_model(original_model, lora_config)\nprint(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:19.379912Z","iopub.execute_input":"2024-09-08T13:26:19.380189Z","iopub.status.idle":"2024-09-08T13:26:20.071223Z","shell.execute_reply.started":"2024-09-08T13:26:19.380158Z","shell.execute_reply":"2024-09-08T13:26:20.070285Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"trainable model parameters: 3538944\nall model parameters: 251116800\npercentage of trainable model parameters: 1.41%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    auto_find_batch_size=True,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=10,\n    per_device_train_batch_size=4,  # batch size for train\n    per_device_eval_batch_size=4,  # batch size for eval\n    fp16=True\n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:20.072500Z","iopub.execute_input":"2024-09-08T13:26:20.072900Z","iopub.status.idle":"2024-09-08T13:26:20.358875Z","shell.execute_reply.started":"2024-09-08T13:26:20.072855Z","shell.execute_reply":"2024-09-08T13:26:20.357855Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.","metadata":{}},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path = \"./peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:26:20.360160Z","iopub.execute_input":"2024-09-08T13:26:20.360475Z","iopub.status.idle":"2024-09-08T13:29:30.038597Z","shell.execute_reply.started":"2024-09-08T13:26:20.360442Z","shell.execute_reply":"2024-09-08T13:29:30.037664Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [320/320 03:08, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('./peft-dialogue-summary-checkpoint-local/tokenizer_config.json',\n './peft-dialogue-summary-checkpoint-local/special_tokens_map.json',\n './peft-dialogue-summary-checkpoint-local/spiece.model',\n './peft-dialogue-summary-checkpoint-local/added_tokens.json',\n './peft-dialogue-summary-checkpoint-local/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n\npeft_model = PeftModel.from_pretrained(\n    peft_model_base, './peft-dialogue-summary-checkpoint-local', is_trainable=False\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:29:30.040038Z","iopub.execute_input":"2024-09-08T13:29:30.040488Z","iopub.status.idle":"2024-09-08T13:29:34.210096Z","shell.execute_reply.started":"2024-09-08T13:29:30.040421Z","shell.execute_reply":"2024-09-08T13:29:34.206935Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:29:34.211782Z","iopub.execute_input":"2024-09-08T13:29:34.212338Z","iopub.status.idle":"2024-09-08T13:29:34.224674Z","shell.execute_reply.started":"2024-09-08T13:29:34.212282Z","shell.execute_reply":"2024-09-08T13:29:34.223496Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"trainable model parameters: 0\nall model parameters: 251116800\npercentage of trainable model parameters: 0.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"index = 42\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids.cpu(), generation_config=GenerationConfig(max_new_tokens=128, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:29:34.226051Z","iopub.execute_input":"2024-09-08T13:29:34.226579Z","iopub.status.idle":"2024-09-08T13:29:36.020533Z","shell.execute_reply.started":"2024-09-08T13:29:34.226539Z","shell.execute_reply":"2024-09-08T13:29:36.019619Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# wants to adjust #Person1#'s life and #Person2# suggests #Person1# be positive and stay healthy.\n---------------------------------------------------------------------------------------------------\nORIGINAL MODEL:\nPerson1#: I don't know how to adjust my life.\n---------------------------------------------------------------------------------------------------\nINSTRUCT MODEL:\nThe person who is talking to me is talking about how she's been coping with her life so far.\n---------------------------------------------------------------------------------------------------\nPEFT MODEL: Person1: I'm not sure how to adjust my life.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids.cuda(), generation_config=GenerationConfig(max_new_tokens=128))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids.cpu(), generation_config=GenerationConfig(max_new_tokens=128))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:29:36.021894Z","iopub.execute_input":"2024-09-08T13:29:36.022270Z","iopub.status.idle":"2024-09-08T13:30:00.511412Z","shell.execute_reply.started":"2024-09-08T13:29:36.022225Z","shell.execute_reply":"2024-09-08T13:30:00.510584Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                            human_baseline_summaries  \\\n0  Ms. Dawson helps #Person1# to write a memo to ...   \n1  In order to prevent employees from wasting tim...   \n2  Ms. Dawson takes a dictation for #Person1# abo...   \n3  #Person2# arrives late because of traffic jam....   \n4  #Person2# decides to follow #Person1#'s sugges...   \n5  #Person2# complains to #Person1# about the tra...   \n6  #Person1# tells Kate that Masha and Hero get d...   \n7  #Person1# tells Kate that Masha and Hero are g...   \n8  #Person1# and Kate talk about the divorce betw...   \n9  #Person1# and Brian are at the birthday party ...   \n\n                            original_model_summaries  \\\n0  #Person1: This should go out as an intra-offic...   \n1  Employees are asked to use the \"Instant Messag...   \n2  Ms. Dawson, I need to take a memo from you. I ...   \n3  People are complaining about the congestion in...   \n4                          Person1 is going to work.   \n5  #Person2: I'm finally here. I've been stuck in...   \n6               Masha and Hero are getting divorced.   \n7  #Person1: I'm so happy for you and Kate. #Pers...   \n8  Masha and Hero are having a separation for 2 m...   \n9                            Brian asks for a drink.   \n\n                            instruct_model_summaries  \\\n0  Employees should receive an intra-office memo ...   \n1  Employees who are using Instant Messaging will...   \n2  Employees who use instant messaging will be pl...   \n3  The traffic jam has caused a lot of congestion...   \n4  #Person1#: It's a good idea to take public tra...   \n5  The traffic jams have caused a lot of congesti...   \n6                        #Person1#: What a surprise!   \n7               Masha and Hero are getting divorced.   \n8                       Masha and Hero are divorced.   \n9                   Brian's birthday was celebrated.   \n\n                                peft_model_summaries  \n0     #Person1#: I need to take a dictation for you.  \n1     #Person1#: I need to take a dictation for you.  \n2     #Person1#: I need to take a dictation for you.  \n3  The traffic jam at the Carrefour intersection ...  \n4  The traffic jam at the Carrefour intersection ...  \n5  The traffic jam at the Carrefour intersection ...  \n6               Masha and Hero are getting divorced.  \n7               Masha and Hero are getting divorced.  \n8               Masha and Hero are getting divorced.  \n9  #Person1#: Happy birthday, Brian. #Person2#: I...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>instruct_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n      <td>#Person1: This should go out as an intra-offic...</td>\n      <td>Employees should receive an intra-office memo ...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting tim...</td>\n      <td>Employees are asked to use the \"Instant Messag...</td>\n      <td>Employees who are using Instant Messaging will...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n      <td>Ms. Dawson, I need to take a memo from you. I ...</td>\n      <td>Employees who use instant messaging will be pl...</td>\n      <td>#Person1#: I need to take a dictation for you.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam....</td>\n      <td>People are complaining about the congestion in...</td>\n      <td>The traffic jam has caused a lot of congestion...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n      <td>Person1 is going to work.</td>\n      <td>#Person1#: It's a good idea to take public tra...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the tra...</td>\n      <td>#Person2: I'm finally here. I've been stuck in...</td>\n      <td>The traffic jams have caused a lot of congesti...</td>\n      <td>The traffic jam at the Carrefour intersection ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>#Person1#: What a surprise!</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n      <td>#Person1: I'm so happy for you and Kate. #Pers...</td>\n      <td>Masha and Hero are getting divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce betw...</td>\n      <td>Masha and Hero are having a separation for 2 m...</td>\n      <td>Masha and Hero are divorced.</td>\n      <td>Masha and Hero are getting divorced.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party ...</td>\n      <td>Brian asks for a drink.</td>\n      <td>Brian's birthday was celebrated.</td>\n      <td>#Person1#: Happy birthday, Brian. #Person2#: I...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-09-08T13:30:00.512591Z","iopub.execute_input":"2024-09-08T13:30:00.512943Z","iopub.status.idle":"2024-09-08T13:30:01.983449Z","shell.execute_reply.started":"2024-09-08T13:30:00.512909Z","shell.execute_reply":"2024-09-08T13:30:01.982429Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2072918276289133, 'rouge2': 0.06266200937586175, 'rougeL': 0.1759943981037212, 'rougeLsum': 0.1770916480073489}\nINSTRUCT MODEL:\n{'rouge1': 0.27864040076056185, 'rouge2': 0.09964820443543847, 'rougeL': 0.22451728254241743, 'rougeLsum': 0.22370589995349044}\nPEFT MODEL:\n{'rouge1': 0.24353538210155856, 'rouge2': 0.1181436538994854, 'rougeL': 0.2225, 'rougeLsum': 0.22342236467236465}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}